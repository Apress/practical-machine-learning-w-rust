\documentclass{book}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage[backend=bibtex, sorting=none, style=numeric-comp, defernumbers]{biblatex}
\usepackage{graphicx}

\addbibresource{\jobname.bib}

\begin{document}
\title{Chapter 5 - Natural Language Processing}
\author {Joydeep Bhattacharjee}

\maketitle

% https://github.com/usamec/cntk-rs/blob/master/examples/sparse_ops_and_word_embeddings.rs
% https://github.com/snipsco/snips-nlu-rs
% https://github.com/snipsco/rustling
% https://github.com/snipsco/rustling-ontology
% https://github.com/bosondata/crfsuite-rs
% https://libraries.io/search?keywords=nlp&languages=Rust
% https://github.com/messense/fasttext-rs

In the previous chapters we took at a look at machine learning algorithms and techniques with the datasources being either numerical or categorical. Generally not much processing is required in those datasets because computers are great with standardised and structured data. When trying to apply those machine learning techniques to human language, it opens up a whole new box of challenges as language is not precise and hold different meaning in different contexts. Even the basic structure changes when we move from one language to another. Hence language needs special consideration during the creation of intelligent applications and this is grouped under the domain of Natural Language Processing(NLP).

In this chapter we will be taking a look at different problems statements in NLP and understand the techniques that go towards solving those specific problems. First we will take a look at sentence classification/ Then we will see how to perform Named Entity Recognition on a text corpus. Finally we will understand how to create a intent inference engine to support a good chatbot. These problems would be described using representative data sets.

\section{Sentence Classification}%
To implement sentence classification in Rust we will be using the FastText library. FastText is a library developed by Facebook for efficient learning of word representations and sentence classification. The premise is to build distributed and distributional word vectors using shallow neural networks. First lets take a look at how to perform sentence classification on a corpus.

To understand how to implement classification we will need to work on a dataset. An interesting classification dataset is the spooky author data set\footnote{https://www.kaggle.com/c/spooky-author-identification}. This data set contains text from works of fiction in the public domain and written by three famous authors: Edgar Allan Poe, HP Lovecraft and Mary Shelley. Download the data and unzip the files in a folder \lstinline{data}. We should now have the training file in the folder.

\begin{lstlisting}[caption={}, basicstyle=\small]
$ head -n1 data/train.csv
"id","text","author"
\end{lstlisting}

So the data has three fields so we will create a struct that reflects data.

\begin{lstlisting}[caption={chapter5\\/fasttext\\-model\\/src\\/main\\.rs}, basicstyle=\small]
#[derive(Debug, Deserialize)]
pub struct SpookyAuthor {
  id: String, text: String, author: String
}
\end{lstlisting}

Now before moving forward, let us talk about the dependencies that we will be using to create a simple fasttext model. We have the usual suspects, \lstinline{csv}, \lstinline{serde} and \lstinline{serde_derive} to parse the csv file and deserialize each record into the struct above. We have \lstinline{rand} to shuffle between the data. stopwords, \lstinline{rust-stemmers} and \lstinline{vtext} will be used to normalise and tokenise the corpus. Of course the crate fasttext to create the fasttext classification model and itertools for some helper functions. Hence we should see the dependencies in the Cargo file as below.

\begin{lstlisting}[caption={chapter5\\/fasttext\\-model\\/Cargo\\.toml}, basicstyle=\small]
[dependencies]
csv = "1.0.7"
serde = "1"
serde_derive = "1"
rand = "0.6.5"
fasttext = "0.4.1"
stopwords = "0.1.0"
vtext = "0.1.0-alpha.1"
rust-stemmers = "1.1.0"
itertools = "0.8.0"
\end{lstlisting}

As usual, use the latest version for the different crates. Once the crates are in the dependencies we should be able to import the relevant modules in our \lstinline{main.rs} file.

\begin{lstlisting}[caption={chapter5\\/fasttext\\-model\\/src\\/main\\.rs}, basicstyle=\small]
extern crate serde;
// This lets us write `#[derive(Deserialize)]`.
#[macro_use]
extern crate serde_derive;

use std::io;
use std::vec::Vec;
use std::error::Error;
use std::io::Write;
use std::fs::File;
use std::collections::HashSet;

use csv;
use rand::thread_rng;
use rand::seq::SliceRandom;

use fasttext::{FastText, Args, ModelName, LossName};
use stopwords::{Spark, Language, Stopwords};
use itertools::Itertools;
use vtext::tokenize::VTextTokenizer;
use rust_stemmers::{Algorithm, Stemmer};
\end{lstlisting}

We will use some constants in our code so defining them here.

\begin{lstlisting}[caption={chapter5\\/fasttext\\-model\\/src\\/main\\.rs}, basicstyle=\small]
const TRAIN_FILE: &str = "data.train";
const TEST_FILE: &str = "data.test";
const MODEL: &str = "model.bin";
\end{lstlisting}

FastText requires the training files to be in a specific format. You will have the labels prefixed by \lstinline{__label__} keyword and then space is used as the delimiter, and once all the labels for the specific sentence or document is done then the rest of the sentence comes after it. Different documents are differentiated by new lines. So we will need to convert the present format to the fasttext format. For that of course we will first need read the \lstinline{train.csv} file to our struct. The idea is that once the raw data is deserialized into the structs, we should be able to implement the needed changes as methods in the struct. So similar to chapter2 we will read the lines in the csv file to a vector of \lstinline{SpookyAuthor} and shuffle the vector for better classification later.

\begin{lstlisting}[caption={chapter5\\/fasttext\\-model\\/src\\/main\\.rs}, basicstyle=\small]
fn main() -> Result<(), Box<Error>> {
  let mut rdr = csv::Reader::from_reader(io::stdin());
  let mut data = Vec::new();
  for result in rdr.deserialize() {
    let r: SpookyAuthor = result?;
    data.push(r); // all the data is pushed to this vector.
  }
  data.shuffle(&mut thread_rng()); // we random shuffle the data
  let test_size: f32 = 0.2; // test size is 20%
  let test_size: f32 = data.len() as f32 * test_size;
  let test_size = test_size.round() as usize;
  let (test_data, train_data) = data.split_at(test_size);
  // rest of the code...
\end{lstlisting}

Since we need to prefix the labels with the \lstinline{__label__} keyword we will implement a \lstinline{into_labels} method for \lstinline{SpookyAuthor}.

\begin{lstlisting}[caption={chapter5\\/fasttext\\-model\\/src\\/main\\.rs}, basicstyle=\small]
impl SpookyAuthor {
  // other methods..

  fn into_labels(&self) -> String {
    match self.author.as_str() {
      "EAP" => String::from("__label__EAP"),
      "HPL" => String::from("__label__HPL"),
      "MWS" => String::from("__label__MWS"),
      l => panic!("
      	Not able to parse the target. \
	Some other target got passed. {:?}", l),
    }
  }

  // other methods..
}
\end{lstlisting}

Now to train a good fasttext model it is advised to perform some text preprocessing on the raw text. Text preprocessing is primarily done to achieve text normalisation, which means to convert specific areas of the text so that text is more conducive for machine learning. The techniques shown in this section are not exhaustive, neither full proof, but are generally adopted in various contexts. Please use all text normalisation techniques in your specific context to achieve optimal results.

We can convert all the text to lowercase. This is mainly applicable in Germanic languages which mainly have the Latin script for writing. A prime example of this is English. In these languages there is a difference between lowercase and uppercase and they are considered different during processing in a computing device. Hence it might be important to convert text to lowercase so that similar text are not considered differently during the learning process during training. But we should be careful when converting all text to lowercase as acronymns and enumerations might lose their meaning altogether when converted to lowercase. A simple way in which to convert to Rust is below.

\begin{lstlisting}[caption={chapter5\\/fasttext\\-model\\/src\\/main\\.rs}, basicstyle=\small]
let lc_text = self.text.to_lowercase();
\end{lstlisting}

Computing devices have traditionally stored documents as a sequence of characters. Even Rust stores a random document as a \lstinline{Vec<u8>}. But such a format does not provide any information to the machine learning algorithm. Hence a document needs to be broken down into \textbf{Liguistically Meaningful Units}. This process is called tokenization. We need to create different tokenization libraries for different languages as all languages are not equal in their organisation of meaning. Generally in English, the meaning resides in the words which are delimited by whitespace so it is relatively easier than other languages such as Chinese, Japanese where orthographies might have no spaces to delimit "words" or "tokens". Even in English, care needs to be taken, for example "New Delhi" is needs to be considered together to store the idea of the place. In Rust, we can use the \lstinline{vtext} crate if we are trying to tokenize on an English sentence.

\begin{lstlisting}[caption={chapter5\\/fasttext\\-model\\/src\\/main\\.rs}, basicstyle=\small]
let tok = VTextTokenizer::new("en");
let tokens: Vec<&str> = tok.tokenize(lc_text.as_str()).collect();
\end{lstlisting}

Sometimes in different languages, we have the word being morphed into different languages rules for example the tense of the sentence. For example depending on the situation the "am", "are", "is" is the same as "be". You might argue that these tense differentiation might be important to the resulting meaning but that might not be the case. The tense information might only be contributing noise to the core understanding. Stemming is an attempt to remove these ambiguities and is implemented through some heuristics that chops off the ends of words based on common prefixes and suffixes. In our case for the English language we can use the \lstinline{rust-stemmers} crate. This library provides rust implementations for some stemmer algorithms written in the snowball language\footnote{\href{}{https://snowballstem.org/algorithms/}}. Below we will use the Stemmer struct to parse the tokens from the vtext tokenizer above and then get the stemmed tokens.

\begin{lstlisting}[caption={chapter5\\/fasttext\\-model\\/src\\/main\\.rs}, basicstyle=\small]
let en_stemmer = Stemmer::create(
  Algorithm::English);
let tokens: Vec<String> = tokens.iter().map(
  |x| en_stemmer.stem(x).into_owned())
  .collect();
let mut tokens: Vec<&str> = tokens.iter().map(
  |x| x.as_str()).collect();
\end{lstlisting}


Lastly, we will go ahead and remove the stopwords. Stopwords are commonly used words that are essentially language constructs and do not contribute in terms of meaning to the sentence. Removal of stopwords can be done using the stopwords package or you can just store a vector of the stopwords and filter out those stopwords from the tokens.

\begin{lstlisting}[caption={chapter5\\/fasttext\\-model\\/src\\/main\\.rs}, basicstyle=\small]
let stops: HashSet<_> = Spark::stopwords(Language::English)
            .unwrap().iter().collect();
// notice that tokens was initialized as mutable
tokens.retain(|s| !stops.contains(s));
\end{lstlisting}

We can now join the tokens and return the whole string.

\begin{lstlisting}[caption={chapter5\\/fasttext\\-model\\/src\\/main\\.rs}, basicstyle=\small]
tokens.iter().join(" ")
\end{lstlisting}

These have been the different preprocessing steps that can be implemented. This list of normalisation techniques are not exhaustive and should be employed depending on the context. For example in a financial setting "₹200" should probably be "two hundred rupees" when normalized. Real world text is quite complicated and might even involve different languages mixed together. In that case text normalisation would need to be more complicated and handle more corner cases for an effective ML model. The above steps when put together in a \lstinline{into_tokens} methods can be seen below.


\begin{lstlisting}[caption={chapter5\\/fasttext\\-model\\/src\\/main\\.rs}, basicstyle=\small]
impl SpookyAuthor {
  pub fn into_tokens(&self) -> String {
    // convert all to lowercase
    let lc_text = self.text.to_lowercase();

    // tokenise the words
    let tok = VTextTokenizer::new("en");
    let tokens: Vec<&str> = tok.tokenize(
      lc_text.as_str()).collect();

    // stem the words
    let en_stemmer = Stemmer::create(Algorithm::English);
    let tokens: Vec<String> = tokens.iter().map(
      |x| en_stemmer.stem(x).into_owned()).collect();
    let mut tokens: Vec<&str> = tokens.iter().map(
      |x| x.as_str()).collect();

    // remove the stopwords
    let stops: HashSet<_> = Spark::stopwords(Language::English)
        .unwrap().iter().collect();
    tokens.retain(|s| !stops.contains(s));

    // join the tokens and return
    tokens.iter().join(" ")
  }
  // remaining code ...
\end{lstlisting}

Once done we should be able to join the labels and the string to achieve the fasttext format. We can then take the joined string and write to a file for the training data.

\begin{lstlisting}[caption={chapter5\\/fasttext\\-model\\/src\\/main\\.rs}, basicstyle=\small]
fn push_training_data_to_file(train_data: &[SpookyAuthor],
    filename: &str) -> Result<(), Box<Error>> {
  let mut f = File::create(filename)?;
  for item in train_data {
    writeln!(f, "{} {}", item.into_labels(), item.into_tokens())?;
  }
  Ok(())
}
\end{lstlisting}

Consider that in listing5 we had split the full data into \lstinline{train_data} and \lstinline{test_data}. We can now pass the \lstinline{train_data} to this function to create the training file.

\begin{lstlisting}[caption={chapter5\\/fasttext\\-model\\/src\\/main\\.rs}, basicstyle=\small]
push_training_data_to_file(train_data.to_owned(),
  TRAIN_FILE)?;
\end{lstlisting}

Similar to the training file we can create the test file as well. Although in the case of the test file we should check that we are not adding the labels as well. Test file should only contain the tokens.

\begin{lstlisting}[caption={chapter5\\/fasttext\\-model\\/src\\/main\\.rs}, basicstyle=\small]
fn push_test_data_to_file(test_data: &[SpookyAuthor],
    filename: &str) -> Result<(), Box<Error>> {
  let mut f = File::create(filename)?;
  for item in test_data {
    writeln!(f, "{}", item.into_tokens())?;
  }
  Ok(())
}

// inside main function ..
push_test_data_to_file(test_data.to_owned(),
  TEST_FILE)?;
\end{lstlisting}

Now that all the preprocessing and data organisation is done we can go ahead with the training process. For that we will create an \lstinline{Args} context and pass the context to the FastText struct for training. The args context will have the input file, since this is a classification problem we will need to define the type of model as supervised and the loss function needs to be softmax.

\begin{lstlisting}[caption={chapter5\\/fasttext\\-model\\/src\\/main\\.rs}, basicstyle=\small]
let mut args = Args::new();
args.set_input(TRAIN_FILE);
args.set_model(ModelName::SUP);
args.set_loss(LossName::SOFTMAX);
let mut ft_model = FastText::new();
ft_model.train(&args).unwrap();
\end{lstlisting}

So once the model is created we can check the accuracy and save the model.

\begin{lstlisting}[caption={chapter5\\/fasttext\\-model\\/src\\/main\\.rs}, basicstyle=\small]
let preds = test_data.iter().map(
  |x| ft_model.predict(x.text.as_str(), 1, 0.0));
let test_labels = test_data.iter().map(
  |x| x.into_labels());
let mut hits = 0;
let mut correct_hits = 0;
let preds_clone = preds.clone();
for (predicted, actual) in preds.zip(test_labels) {
  let predicted = predicted?;

  // only taking the first value.
  let predicted = &predicted[0];
  if predicted.clone().label == actual {
    correct_hits += 1;
  }
  hits += 1;
}
assert_eq!(hits, preds_clone.len());
println!("accuracy={} ({}/{} correct)",
  correct_hits as f32 / hits as f32,
  correct_hits, preds_clone.len());
ft_model.save_model(MODEL)?;
\end{lstlisting}

As this model is built in the \lstinline{fasttext} code library, this conforms to the general \lstinline{fasttext} specification and can be used by a different \lstinline{fasttext} application as well. For example if we load the model using the original \lstinline{fasttext} application it works just fine.

\begin{lstlisting}[caption={},basicstyle=\small]
$ ./fasttext predict ../model.bin -
I love ghosts.
__label__EAP
^C
\end{lstlisting}

where fasttext is the application created from compiling the fasttext original code base \footnote{\href{}{https://github.com/facebookresearch/fastText}}.

\label{sec:Sentence Classification}

\section{Named Entity Recognition}%
One of the core areas of implementation of natural language processing is Named Entity Recognition, where entities that are present in the text are classified into predefined categories. These categories are context and problem dependent. For example, a travel organization may be interested in the cities, dates. You could argue that this can be done using regex, but if you are a growing company, it would be very difficult to scale up such an operation. Also going through the NER route adds a wealth of semantic knowledge to the content and helps to understand the subject of any given text.

One of the popular algorithms that are used for NER tasks are Conditional Random Fields. CRF's are essentially classifiers which use contextual information from previous labels, thus increasing the amount of information that the label has to make a good prediction.

To perform CRF effectively the input text would need to be chunked correctly. Text chunking divides each sentence into syntactically correlated parts of words. For example the sentence "Rust is great for machine learning" can be divided as follows:

[NP Rust][VP is][NP great][PP for][NP machine learning]

In this example, NP stands for a noun phrase, VP for a verb phrase, and PP for a prepositional phrase. This task is formalized as a sequential labeling task in which a sequence of tokens tokens in a text is assigned with a sequence of labels. In order to present a chunk the IOB2 notation is used. The beginning of a chunk is given by B-label, inside of the chunk is given by I-label and others are defined as O. Chunking is mostly a manual task and there are some popular annotation tools that make it a little simpler for the user.

\begin{itemize}
	\item GATE - General Architecture and Text Engineering is 15+ years old, free and open source
	\item Anafora is free and open source, web-based raw text annotation tool
	\item brat - brat rapid annotation tool is an online environment for collaborative text annotation
	\item tagtog - a proprietary tool costing money.
	\item prodigy is an annotation tool powered by active learning and costs money.
	\item LightTag is a hosted and managed text annotation tool for team and costs money.\footnote{\href{}{https://github.com/keon/awesome-nlp\#annotation-tools}}
\end{itemize}

We will on the other hand use a data set which has been annotated for us. This is can be downloaded from the kaggle website with this link \\
\href{}{https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus}. Download the ner.csv file and keep it in the data folder. We can see that the data has a lot of fields and this is a good guideline for the type of fields that should be there in an NER data set. Lets take a look at the fields.

\begin{itemize}
	\item lemma - Lemma of a token in sentence
	\item next-lemma Lemma of next token in sentence
	\item next-next-lemma Lemma of token at +2nd position to the current token in sentence
	\item next-next-pos POS tag of token at +2nd position to the current token in sentence
	\item next-next-shape Shape of token at +2nd position to the current token in sentence
	\item next-next-word Token at +2nd position to the current token in sentence
	\item next-pos POS tag of the next(+1 position) token
	\item next-shape Shape of the next(+1 position) token
	\item next-word Next(+1 position) token
	\item pos POS tag of current token
	\item prev-iob IOB annotation of previous token
	\item prev-lemma Lemma of previous token
	\item prev-pos POS tag for previous token
	\item prev-prev-iob IOB annotation of token at -2nd position to the current token in sentence
	\item prev-prev-lemma Lemma of token at -2nd position to the current token in sentence
	\item prev-prev-pos POS tag of token at -2nd position to the current token in sentence
	\item prev-prev-shape Shape of token at -2nd position to the current token in sentence
	\item prev-prev-word Token at -2nd position to the current token in sentence
	\item prev-shape Shape of previous(-1 position to current token) token
	\item prev-word Previous word(-1 position to current token)
	\item \lstinline{sentence_idx} Sentence Index(Tokens having same index belongs to same sentence)
	\item shape Shape of the token in sentence
	\item word Often termed as Token
	\item tag IOB annotation of current token
\end{itemize}

For our example though we will only be using the lemma and next-lemma to have an understanding of how to create an NER model using crf-suite. In a production environment though you should use all the above features and all the other features that you can integrate.

\begin{lstlisting}[caption={chapter5\\/crfsuite\\-model\\/src\\/main\\.rs}, basicstyle=\small]
#[derive(Debug, Deserialize, Clone)]
pub struct NER {
  lemma: String,
  #[serde(rename = "next-lemma")]
  next_lemma: String,
  word: String,
  tag: String
}
\end{lstlisting}

Before we go ahead lets talk about the dependencies that we will need for creating the model.

\begin{lstlisting}[caption={chapter5\\/crfsuite\\-model\\/Cargo\\.toml}, basicstyle=\small]
[dependencies]
csv = "1.0.7"
serde = "1"
serde_derive = "1"
rand = "0.6.5"
crfsuite = "0.2.6"
\end{lstlisting}

Once these are in the toml file we should be able to import the relevant modules in our main module.

\begin{lstlisting}[caption={chapter5\\/crfsuite\\-model\\/src\\/main\\.rs}, basicstyle=\small]
extern crate serde;
#[macro_use]
extern crate serde_derive;

use std::io;
use std::vec::Vec;
use std::error::Error;

use csv;
use rand;
use rand::thread_rng;
use rand::seq::SliceRandom;

# below are the new ones and useful for NER
use crfsuite::{Model, Attribute, CrfError};
use crfsuite::{Trainer, Algorithm, GraphicalModel};
\end{lstlisting}

Similar to what we have seen before we can now create a function that reads the data from the standard input and store it in a vector of the data struct that we have, in this case that being \lstinline{Vec<NER>}

\begin{lstlisting}[caption={chapter5\\/crfsuite\\-model\\/src\\/main\\.rs}, basicstyle=\small]
fn get_data() -> Result<Vec<NER>, Box<Error>> {
  let mut rdr = csv::Reader::from_reader(io::stdin());
  let mut data = Vec::new();
  for result in rdr.deserialize() {
    let r: NER = result?;
    data.push(r);
  }
  data.shuffle(&mut thread_rng());
  Ok(data)
}
\end{lstlisting}

Given that now we have the data we should be able to split the data into train and test, so that we can check the accuracy on out of sample data. This is the same code that we have seen before in other modules and chapters.

\begin{lstlisting}[caption={chapter5\\/crfsuite\\-model\\/src\\/main\\.rs}, basicstyle=\small]
fn split_test_train(data: &[NER],
                    test_size: f32)
		    -> (Vec<NER>, Vec<NER>) {
  let test_size: f32 = data.len() as f32 * test_size;
  let test_size = test_size.round() as usize;
  let (test_data, train_data) = data.split_at(test_size);
  (test_data.to_vec(), train_data.to_vec())
}
\end{lstlisting}

Now comes the interesting part. Given the data, we will need to extract each item in the data to a vector of \lstinline{Attribute}s, provided by the \lstinline{crf_suite} crate so that the training module is able to train on the data set. This \lstinline{Attribute} will contain the token and the value of the token which is the weightage that the token has in the sequence. In the below code weightage of 1.0 is given for the target word and 0.5 for the next word but you can play with these weightage and see which gives a better result. The labels can be a \lstinline{Vec<String>} as expected.

\begin{lstlisting}[caption={chapter5\\/crfsuite\\-model\\/src\\/main\\.rs}, basicstyle=\small]
fn create_xseq_yseq(
    data: &[NER])
    -> (Vec<Vec<Attribute>>, Vec<String>) {
  let mut xseq = vec![];
  let mut yseq = vec![];
  for item in data {
    let seq = vec![Attribute::new(item.lemma.clone(), 1.0),
      Attribute::new(item.next_lemma.clone(), 0.5)]; // higher weightage for the mainword.
    xseq.push(seq);
    yseq.push(item.tag.clone());
  }
  (xseq, yseq)
}
\end{lstlisting}

We can now create a function that will do the model prediction given x-sequence and y-sequence. In this case after training the model saves in a file which is determined by \lstinline{model_name}.

\begin{lstlisting}[caption={chapter5\\/crfsuite\\-model\\/src\\/main\\.rs}, basicstyle=\small]
fn crfmodel_training(xseq: Vec<Vec<Attribute>>,
                     yseq: Vec<String>,
		     model_name: &str)
		     -> Result<(), Box<CrfError>> {
  let mut trainer = Trainer::new(true); // verbose is true
  trainer.select(Algorithm::AROW, GraphicalModel::CRF1D)?;
  trainer.append(&xseq, &yseq, 0i32)?;
  trainer.train(model_name, -1i32)?; // using all instances for training.
  Ok(())
}
\end{lstlisting}

In the above model we used the Adaptive regularization of weight vector. The different algorithms that we can use are:

\begin{itemize}
	\item Gradient descent using the L-BFGS method: It is a way of finding the local minimum of objective function, making use of objective function values and the gradient of the objective function.
	\item Stochastic Gradient Descent with L2 regularization term
	\item Averaged Perceptron
	\item Passive Aggressive
	\item Adaptive Regularization Of Weight Vector: This algorithm combines large margin training, confidence weighting and the capacity to handle non-separable data. AROW performs adaptive regularization of the prediction function upon seeing each new instance, allowing it to perform especially well in the presence of label noise.
\end{itemize}

Once training is done, we can load the model from file and run predictions on it.

\begin{lstlisting}[caption={chapter5\\/crfsuite\\-model\\/src\\/main\\.rs}, basicstyle=\small]
fn model_prediction(xtest: Vec<Vec<Attribute>>,
                    model_name: &str)
		    -> Result<Vec<String>, Box<CrfError>>{
  let model = Model::from_file(model_name)?;
  let mut tagger = model.tagger()?;
  let preds = tagger.tag(&xtest)?;
  Ok(preds)
}
\end{lstlisting}

We can use this \lstinline{model_prediction} function and see how our model fared on the test data. To do this we need to have the accuracy function. The accuracy is the same as the accuracy functions that we have seen in the previous chapters.

\begin{lstlisting}[caption={chapter5\\/crfsuite\\-model\\/src\\/main\\.rs}, basicstyle=\small]
fn check_accuracy(preds: &[String], actual: &[String]) {
  let mut hits = 0;
  let mut correct_hits = 0;
  for (predicted, actual) in preds.iter().zip(actual) {
    if actual != "O" { // will not consider the other category as it bloats the accuracy.
      if predicted == actual && actual != "O" {
        correct_hits += 1;
      }
      hits += 1;
    }
  }
  println!("accuracy={} ({}/{} correct)",
    correct_hits as f32 / hits as f32,
    correct_hits,
    hits);
}
\end{lstlisting}

Notice the difference, we will not consider the values where the actual is "O". That is because this label means other and most of the labels would be others. So considering this as part of the accuracy values would essentially bloat up our accuracy and give accuracy results better than they actually are. A more general way in which the accuracy of the CRF model is determined using precision and recall and the F1 score, which can also be implemented in Rust in a similar manner.

Now that we have all the relevant functions we can stitch all those functions in our main function.

\begin{lstlisting}[caption={chapter5\\/crfsuite\\-model\\/src\\/main\\.rs}, basicstyle=\small]
fn main() {
  let data = get_data().unwrap();
  let (test_data, train_data) = split_test_train(
  	&data, 0.2);
  let (xseq_train, yseq_train) = create_xseq_yseq(
  	&train_data);
  let (xseq_test, yseq_test) = create_xseq_yseq(
  	&test_data);
  crfmodel_training(xseq_train,
  	yseq_train,
	"rustml.crfsuite").unwrap();
  let preds = model_prediction(xseq_test,
  	"rustml.crfsuite").unwrap();
  check_accuracy(&preds, &yseq_test);
}
\end{lstlisting}

Named Entity Recognition are an important and interesting set of problem in NLP and the crfsuite is an excellent crate to create a CRF model for NER.

In this section, we looked at how to use to create a CRF model for named entity recognition in Rust. We also looked at how to mould incoming data set so that it is conducive to the CRF suite model. Finally we took a look at one of the ways in which accuracy for a CRF model can be decided.

\label{sec:Named Entity Recognition}

\section{Chatbots and Natural Language Understanding}%
While chatbots is a complex topic and involve a lot of integration and engineering for a production system, one of the core areas that power a chatbot is for the underlying system to have some level of natural language understanding. NLU is needed to determine a user's intention and to extract information from an utterance and to carry on a conversation with the user in order to execute and complete a task.

\paragraph{Intent Recognition}%
Most intents are simple discrete tasks like "Find Product", "Transfer Funds", "Book Flight" and are typically described with the verb and noun combination. These types of intents will initiate a dialogue with the user to capture more information, to fetch and update data from remote systems and to inform the user of its progress.

The goal of intent recognition is to match a user utterance with its correctly intended task or question. Intent parsing of user utterance can be done in Rust using the crate \lstinline{snips-nlu-lib}. The caveat though is that this library can only be used for the inference part and training needs to be done in python.
\label{par:intent_recognition}

Now before moving ahead ensure that you have clang installed in your machine. For an ubuntu machine, installation of clang can be done using the below command.

\begin{lstlisting}[caption={}]
$ sudo apt update
$ sudo apt install clang
\end{lstlisting}

Steps to create an snips model. Since the snips model is a python model, we can create a virtual environment and install the package.

\begin{lstlisting}[caption={}, basicstyle=\small]
$ python3 -m venv venv
$ source venv/bin/activate
$ pip install snips-nlu
\end{lstlisting}

To download the model we will need to download the english model. Other models as of this writing are: German, Spanish, French, Italian, Japanese, Korean, Portuguese (one for Brazil dialect and one for Portugal dialect)\footnote{\href{}{https://snips-nlu.readthedocs.io/en/latest/languages.html}}. Implementing a different language involves changes in a lot of repositories which is not very simple and hence the snips team is working on making the whole process simpler\footnote{\href{}{https://github.com/snipsco/snips-nlu-language-resources/issues/12\#issuecomment-433325114}}. If downloading the english model does not work for the remaining steps, we can also download all the resources and try again.

\begin{lstlisting}[caption={}, basicstyle=\small]
$ python -m snips_nlu download en
# or we can try
$ python -m snips-nlu download-all-languages
\end{lstlisting}

We can now create a dataset.yaml for our different types of utterances.

\begin{lstlisting}[caption={}, basicstyle=\tiny]
(venv) $ cat dataset.yaml
# turnLightOn intent
---
type: intent
name: turnLightOn
slots:
  - name: room
    entity: room
utterances:
  - Turn on the lights in the [room](kitchen)
  - give me some light in the [room](bathroom) please
  - Can you light up the [room](living room) ?
  - switch the [room](bedroom)'s lights on please

# turnLightOff intent
---
type: intent
name: turnLightOff
slots:
  - name: room
    entity: room
utterances:
  - Turn off the lights in the [room](entrance)
  - turn the [room](bathroom)'s light out please
  - switch off the light the [room](kitchen), will you?
  - Switch the [room](bedroom)'s lights off please

# setTemperature intent
---
type: intent
name: setTemperature
slots:
  - name: room
    entity: room
  - name: roomTemperature
    entity: snips/temperature
utterances:
  - Set the temperature to [roomTemperature](19 degrees) in the [room](bedroom)
  - please set the [room](living room)'s temperature to [roomTemperature](twenty two degrees celsius)
  - I want [roomTemperature](75 degrees fahrenheit) in the [room](bathroom) please
  - Can you increase the temperature to [roomTemperature](22 degrees) ?

# room entity
---
type: entity
name: room
automatically_extensible: no
values:
- bedroom
- [living room, main room, lounge]
- [garden, yard, backyard]
\end{lstlisting}

We would now need to create our \lstinline{dataset.json} file from the utterances yaml file.

\begin{lstlisting}[caption={}, basicstyle=\small]
snips-nlu generate-dataset en dataset.yaml > dataset.json
\end{lstlisting}

Once the \lstinline{dataset.json} file is created we should be able to train a model.

\begin{lstlisting}[caption={}, basicstyle=\small]
$ snips-nlu train dataset.json snips.model -v
\end{lstlisting}

The output of the above command should be something like below.

\begin{lstlisting}[caption={}, basicstyle=\tiny]
Create and train the engine...
[INFO][21:55:51.091][snips_nlu.intent_parser.deterministic_intent_parser]: Fitting deterministic parser...
[INFO][21:55:51.103][snips_nlu.intent_parser.deterministic_intent_parser]: Fitted deterministic parser in 0:00:00.011588
[INFO][21:55:51.103][snips_nlu.intent_parser.probabilistic_intent_parser]: Fitting probabilistic intent parser...
[DEBUG][21:55:51.103][snips_nlu.intent_classifier.log_reg_classifier]: Fitting LogRegIntentClassifier...
[DEBUG][21:55:51.616][snips_nlu.intent_classifier.log_reg_classifier]: Top 50 features weights by intent:


<training on different intents>
...
< training on transition weights and feature weights.>

[DEBUG][21:55:57.292][snips_nlu.slot_filler.crf_slot_filler]: Fitted CRFSlotFiller in 0:00:01.482648
[DEBUG][21:55:57.293][snips_nlu.intent_parser.probabilistic_intent_parser]: Fitted slot fillers in 0:00:05.671146
[INFO][21:55:57.294][snips_nlu.intent_parser.probabilistic_intent_parser]: Fitted probabilistic intent parser in 0:00:06.190487
[INFO][21:55:57.294][snips_nlu.nlu_engine.nlu_engine]: Fitted NLU engine in 0:00:07.315197
Persisting the engine...
Saved the trained engine 

\end{lstlisting}

\subsection{Building an inference engine}%
Once the training is done, we should be able to use the model in our application. As an example application, we will be creating a simple api that would take a sentence as an input and provide the parsed intents in the sentence.

As for the dependencies, we would be using the \lstinline{snips-nlu-lib} crate for the snips model inference. \lstinline{serde}, \lstinline{serde_json} and \lstinline{serde_derive} would be used for serializing and deserializing the incoming request. For creating the web-app, we would be using the rocket web framework. The api from the rocket framework is inspired out of great web-frameworks such as Rails, Flask, Bottle and Yesod and hence it is quite fun to create applications in Rocket. These are our primary dependencies, so we should have the cargo file similar to below.

\begin{lstlisting}[caption={chapter5\\/snips\\-model\\/Cargo\\.toml}, basicstyle=\small]
[package]
name = "snips-model"
version = "0.1.0"
authors = ["put your name here."]
edition = "2018"

[dependencies]
snips-nlu-lib = { git = "https://github.com/snipsco/snips-nlu-rs", branch = "master" }
rocket = "0.4.0"
rocket_contrib = "0.4.0"
serde = "1.0"
serde_json = "1.0"
serde_derive = "1.0"
\end{lstlisting}

Note that we are using the github link directly for the snips library as that works the best for now.

We can now import the relevant modules from the dependencies in our main file.

\begin{lstlisting}[caption={chapter5\\/snips\\-model\\/src\\/main\\.rs}, basicstyle=\small]
#![feature(proc_macro_hygiene, decl_macro)]

#[macro_use] extern crate rocket;
#[macro_use] extern crate rocket_contrib;
#[macro_use] extern crate serde_derive;
extern crate snips_nlu_lib;

use std::sync::Mutex;

use snips_nlu_lib::SnipsNluEngine;
use rocket::{Rocket, State};
use rocket_contrib::json::Json;
\end{lstlisting}

We can now write a small function to see if our setup till now is working or not.

\begin{lstlisting}[caption={chapter5\\/snips\\-model\\/src\\/main\\.rs}, basicstyle=\small]
#[get("/")]
fn hello() -> &'static str {
  "Hello, from snips model inference!"
}
\end{lstlisting}

To activate this endpoint we will need to do a little setup and launch the app.

\begin{lstlisting}[caption={chapter5\\/snips\\-model\\/src\\/main\\.rs}, basicstyle=\small]
fn rocket() -> Rocket {
    rocket::ignite()
        .mount("/", routes![hello])
}

fn main() {
    rocket().launch();
}
\end{lstlisting}

Run the application using \lstinline{cargo run}. This should build all the dependencies and then if everything had been fine till now should run web application and you should be able to see the following output.

\begin{lstlisting}[caption={}, basicstyle=\small]
➜  snips-model git:(master) ✗ cargo run
   Compiling snips-model v0.1.0 (...)
    Finished dev [unoptimized + debuginfo] target(s) in 1m 39s
     Running `target/debug/snips-model`

Loading the nlu engine...
🔧 Configured for development.
    => address: localhost
    => port: 8000
    => log: normal
    => workers: 8
    => secret key: generated
    => limits: forms = 32KiB
    => keep-alive: 5s
    => tls: disabled
🛰  Mounting /:
    => GET / (hello)
🚀 Rocket has launched from http://localhost:8000
\end{lstlisting}
\label{sub:building_an_inference_engine}

And we should be able to make a simple call to the webserver.

\begin{lstlisting}[caption={}, basicstyle=\small]
$ curl localhost:8000/
Hello, from snips model inference!
\end{lstlisting}

Now moving into the main inference part. We will try to create a function that will handle the post request. Hence we will create a struct \lstinline{Message} that will hold the incoming request data, and a function \lstinline{infer}, that will handle the inference part. So writing code for handling the incoming function will look something like this.

\begin{lstlisting}[caption={chapter5\\/snips\\-model\\/src\\/main\\.rs}, basicstyle=\small]
#[derive(Serialize, Deserialize)]
struct Message {
    contents: String
}

#[post("/infer", format = "json", data = "<message>")]
fn infer(message: Json<Message>,
	<remaining arguments>) -> OutputType {
    let query = message.0.contents; // to get the query from the incoming data
    // remaining code ...
}

fn rocket() -> Rocket {
    // Have Rocket manage the engine to be passed to the functions.
    rocket::ignite()
        .mount("/", routes![hello, infer])
}

fn main() {
    rocket().launch();
}
\end{lstlisting}

We will now need to load the engine, an object of type \lstinline{SnipsNluEngine}, run the inference based on the incoming query and return the result. Since this is dependent on the user query sentence a naive way might be to put all the code in the \lstinline{infer} function. This though is not ideal as creating the engine from the model is quite expensive and having to do that all over again for each request is not ideal. Hence we need a mechanism to load the model and create the engine once at the start of the app initiation and then use the same model for each request inference cycle. Since this model will be shared between threads in the Rocket app, we cannot just create a global variable or something similar. An elegant solution for this is by using rust mutexes\footnote{\href{}{https://doc.rust-lang.org/std/sync/struct.Mutex.html}}. This is a highly useful mutual exclusion primitive implemented as a struct to be used for protecting shared data.

So we will wrap the engine as a mutex.

\begin{lstlisting}[caption={chapter5\\/snips\\-model\\/src\\/main\\.rs}, basicstyle=\small]
type Engine = Mutex<SnipsNluEngine>;
\end{lstlisting}

A different function will be defined for creating the engine. This function will be called before initializing the Rocket app, and the engine object would be passed to the rocket app to be managed by it.

\begin{lstlisting}[caption={chapter5\\/snips\\-model\\/src\\/main\\.rs}, basicstyle=\small]
fn init_engine() -> SnipsNluEngine {
  let engine_dir = "path to snips model/snips.model";
  println!("\nLoading the nlu engine...");
  let engine = SnipsNluEngine::from_path(engine_dir).unwrap();
  engine
}

fn rocket() -> Rocket {
  // load the snips ingerence engine.
  let engine = init_engine();

  // Have Rocket manage the engine to be passed to the functions.
  rocket::ignite()
    .manage(Mutex::new(engine)) // we are letting rocket manage the state of engine.
    .mount("/", routes![hello, infer])
}
\end{lstlisting}

Once our rocket app has access to the engine object, we can call it in our functions, \lstinline{infer} in this case, and get the intents from the user provided document.

\begin{lstlisting}[caption={chapter5\\/snips\\-model\\/src\\/main\\.rs}, basicstyle=\small]
#[post("/infer", format = "json", data = "<message>")]
fn infer(message: Json<Message>, engine: State<Engine>) -> String {
  let query = message.0.contents;

  // locking the mutex so that nothing else changes it and we can utilise the underlying resource
  let engine = engine.lock().unwrap();

  // get the intents
  let result = engine.get_intents(query.trim()).unwrap();

  // serialize the result as a json string.
  let result_json = serde_json::to_string_pretty(&result).unwrap();
  result_json
}
\end{lstlisting}

Building the intent inference is done. We can now run the app using \lstinline{cargo run}. So apart from the other output that we have seen before when running the app, we should see the \lstinline{infer} endpoint being loaded as well. We should also see the print statement that tells us that the model has been loaded from the \lstinline{init_engine} function.

\begin{lstlisting}[caption={}, basicstyle=\small]
Loading the nlu engine...
// other output...
🛰  Mounting /:
    => GET / (hello)
    => POST /infer application/json (infer)
🚀 Rocket has launched from http://localhost:8000
\end{lstlisting}

If the \lstinline{infer} endpoint is loaded correctly, we should now be able to make requests to the app and get the result.

\begin{lstlisting}[caption={}, basicstyle=\tiny]
$ curl --header "Content-Type: application/json" \
	--request POST \
	--data '{"contents":"set the temperature to 23 degrees in the bedroom"}' \
	localhost:8000/infer
[
  {
    "intentName": "setTemperature",
    "confidenceScore": 1.0
  },
  {
    "intentName": null,
    "confidenceScore": 0.291604
  },
  {
    "intentName": "turnLightOn",
    "confidenceScore": 0.08828778
  },
  {
    "intentName": "turnLightOff",
    "confidenceScore": 0.070788406
  }
]
\end{lstlisting}

One of the important problem statements in NLP is chatbots and to build chatbots, we need a good intent inference engine. In this section, we took a look at how to train a snips model and use the model in a simple webapp for inferring the intents in a user specified query.

\label{sec:chatbots_and_natural_language_understanding}

\section{Conclusion}%

This chapter introduced you to different interesting problems in NLP. The chapter started with text classification and how to build a classifier using the fasttext algorithm. It also had common text preprocessing steps that can be done in Rust. Then the chapter went on with a look at performing named entity recognition on a corpus and how to create a CRF model using the crfsuite crate to perform the NER task. Lastly we looked at creating an intent inference web-application using the snips library.

In the next chapter you will learn about creating computer vision models and running image classifiers.
\label{sec:conclusion}

\printbibliography
\nocite{*}
\end{document}
